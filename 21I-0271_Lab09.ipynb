{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Arsalan Ahmed\n",
    "### Roll No: 21I-0271\n",
    "#### Lab 08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 22:23:00.953699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import errno\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S4fQktaa67wX"
   },
   "outputs": [],
   "source": [
    "def to_np(var):\n",
    "    \"\"\"Exports torch.Tensor to Numpy array.\n",
    "    \"\"\"\n",
    "    return var.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def create_folder(folder_path):\n",
    "    \"\"\"Create a folder if it does not exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(folder_path)\n",
    "    except OSError as _e:\n",
    "        if _e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "\n",
    "def clear_folder(folder_path):\n",
    "    \"\"\"Clear all contents recursively if the folder exists.\n",
    "    Create the folder if it has been accidently deleted.\n",
    "    \"\"\"\n",
    "    create_folder(folder_path)\n",
    "    for the_file in os.listdir(folder_path):\n",
    "        _file_path = os.path.join(folder_path, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(_file_path):\n",
    "                os.unlink(_file_path)\n",
    "            elif os.path.isdir(_file_path):\n",
    "                shutil.rmtree(_file_path)\n",
    "        except OSError as _e:\n",
    "            print(_e)\n",
    "\n",
    "\n",
    "class StdOut(object):\n",
    "    \"\"\"Redirect stdout to file, and print to console as well.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_file):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(output_file, \"a\")\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.terminal.flush()\n",
    "        self.log.write(message)\n",
    "        self.log.flush()\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AczRCzbE5HG",
    "outputId": "9bde671a-a2d3-42ac-aac9-84d624f0b286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to output/log.txt\n",
      "\n",
      "PyTorch version: 2.1.0+cu118\n",
      "CUDA version: 11.8\n",
      "\n",
      "Random Seed:  1\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 91733251.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/Data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 111337953.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/Data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1648877/1648877 [00:00<00:00, 138038989.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/Data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to /root/Data/mnist/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 21405088.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /root/Data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to /root/Data/mnist/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Epoch 0 [0/118] loss_D_real: 1.1193 loss_D_fake: 1.0719 loss_G: 2.5317\n",
      "Epoch 0 [100/118] loss_D_real: 0.0615 loss_D_fake: 0.0426 loss_G: 4.4436\n",
      "Epoch 1 [0/118] loss_D_real: 0.0521 loss_D_fake: 0.0385 loss_G: 5.2617\n",
      "Epoch 1 [100/118] loss_D_real: 0.0048 loss_D_fake: 0.9412 loss_G: 10.6261\n",
      "Epoch 2 [0/118] loss_D_real: 0.0707 loss_D_fake: 0.0351 loss_G: 4.2099\n",
      "Epoch 2 [100/118] loss_D_real: 0.0646 loss_D_fake: 0.0602 loss_G: 3.5382\n",
      "Epoch 3 [0/118] loss_D_real: 0.1423 loss_D_fake: 0.0344 loss_G: 2.1584\n",
      "Epoch 3 [100/118] loss_D_real: 0.1247 loss_D_fake: 0.1219 loss_G: 2.7910\n",
      "Epoch 4 [0/118] loss_D_real: 0.0266 loss_D_fake: 0.4719 loss_G: 5.1799\n",
      "Epoch 4 [100/118] loss_D_real: 0.2164 loss_D_fake: 0.1122 loss_G: 1.9734\n",
      "Epoch 5 [0/118] loss_D_real: 0.1325 loss_D_fake: 0.1699 loss_G: 2.5708\n",
      "Epoch 5 [100/118] loss_D_real: 0.2400 loss_D_fake: 0.2915 loss_G: 2.1150\n",
      "Epoch 6 [0/118] loss_D_real: 0.1796 loss_D_fake: 0.3052 loss_G: 2.2188\n",
      "Epoch 6 [100/118] loss_D_real: 0.1360 loss_D_fake: 0.2763 loss_G: 2.2458\n",
      "Epoch 7 [0/118] loss_D_real: 0.1514 loss_D_fake: 0.1640 loss_G: 2.3155\n",
      "Epoch 7 [100/118] loss_D_real: 0.0401 loss_D_fake: 0.4735 loss_G: 3.6566\n",
      "Epoch 8 [0/118] loss_D_real: 1.1698 loss_D_fake: 0.0400 loss_G: 0.6862\n",
      "Epoch 8 [100/118] loss_D_real: 0.3975 loss_D_fake: 0.0727 loss_G: 2.0482\n",
      "Epoch 9 [0/118] loss_D_real: 0.9701 loss_D_fake: 0.0626 loss_G: 1.0728\n",
      "Epoch 9 [100/118] loss_D_real: 0.0532 loss_D_fake: 0.7062 loss_G: 4.0850\n",
      "Epoch 10 [0/118] loss_D_real: 0.0328 loss_D_fake: 1.3186 loss_G: 4.0034\n",
      "Epoch 10 [100/118] loss_D_real: 0.8452 loss_D_fake: 0.2464 loss_G: 1.2187\n",
      "Epoch 11 [0/118] loss_D_real: 0.2636 loss_D_fake: 0.1633 loss_G: 2.3105\n",
      "Epoch 11 [100/118] loss_D_real: 0.0768 loss_D_fake: 0.2165 loss_G: 3.7191\n"
     ]
    }
   ],
   "source": [
    "CUDA = True     # Change to False for CPU training\n",
    "DATA_PATH = '~/Data/mnist'\n",
    "#DATA_PATH = '/media/john/FastData/CelebA'\n",
    "# DATA_PATH = '/media/john/FastData/lsun'\n",
    "OUT_PATH = 'output'\n",
    "LOG_FILE = os.path.join(OUT_PATH, 'log.txt')\n",
    "BATCH_SIZE = 512        # Adjust this value according to your GPU memory\n",
    "IMAGE_CHANNEL = 1\n",
    "# IMAGE_CHANNEL = 3\n",
    "Z_DIM = 100\n",
    "G_HIDDEN = 64\n",
    "X_DIM = 64\n",
    "D_HIDDEN = 64\n",
    "EPOCH_NUM = 12\n",
    "REAL_LABEL = 1\n",
    "FAKE_LABEL = 0\n",
    "lr = 2e-4\n",
    "seed = 1            # Change to None to get different results at each run\n",
    "\n",
    "clear_folder(OUT_PATH)\n",
    "print(\"Logging to {}\\n\".format(LOG_FILE))\n",
    "sys.stdout = StdOut(LOG_FILE)\n",
    "CUDA = CUDA and torch.cuda.is_available()\n",
    "print(\"PyTorch version: {}\".format(torch.__version__))\n",
    "if CUDA:\n",
    "    print(\"CUDA version: {}\\n\".format(torch.version.cuda))\n",
    "\n",
    "if seed is None:\n",
    "    seed = np.random.randint(1, 10000)\n",
    "print(\"Random Seed: \", seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if CUDA:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "cudnn.benchmark = True      # May train faster but cost more memory\n",
    "\n",
    "dataset = dset.MNIST(root=DATA_PATH, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                      transforms.Resize(X_DIM),\n",
    "                      transforms.ToTensor(),\n",
    "                      transforms.Normalize((0.5,), (0.5,))\n",
    "                      ]))\n",
    "\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if CUDA else \"cpu\")\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"custom weights initialization\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1st layer\n",
    "            nn.ConvTranspose2d(Z_DIM, G_HIDDEN * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 8),\n",
    "            nn.ReLU(True),\n",
    "            # 2nd layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 8, G_HIDDEN * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 4),\n",
    "            nn.ReLU(True),\n",
    "            # 3rd layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 4, G_HIDDEN * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN * 2),\n",
    "            nn.ReLU(True),\n",
    "            # 4th layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN * 2, G_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(G_HIDDEN),\n",
    "            nn.ReLU(True),\n",
    "            # output layer\n",
    "            nn.ConvTranspose2d(G_HIDDEN, IMAGE_CHANNEL, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # 1st layer\n",
    "            nn.Conv2d(IMAGE_CHANNEL, D_HIDDEN, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 2nd layer\n",
    "            nn.Conv2d(D_HIDDEN, D_HIDDEN * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 3rd layer\n",
    "            nn.Conv2d(D_HIDDEN * 2, D_HIDDEN * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 4th layer\n",
    "            nn.Conv2d(D_HIDDEN * 4, D_HIDDEN * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(D_HIDDEN * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # output layer\n",
    "            nn.Conv2d(D_HIDDEN * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = Discriminator().to(device)\n",
    "netD.apply(weights_init)\n",
    "print(netD)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "viz_noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1, device=device)\n",
    "\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        x_real = data[0].to(device)\n",
    "\n",
    "        real_label = torch.full((x_real.size(0),), REAL_LABEL, device=device,dtype=torch.float32)\n",
    "        fake_label = torch.full((x_real.size(0),), FAKE_LABEL, device=device,dtype=torch.float32)\n",
    "\n",
    "        # Update D with real data\n",
    "        netD.zero_grad()\n",
    "        dreal= netD(x_real).view(-1)\n",
    "        loss_D_real= criterion(dreal, real_label)\n",
    "        loss_D_real.backward()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update D with fake data\n",
    "        z_noise = torch.randn(x_real.size(0), Z_DIM, 1, 1, device=device)\n",
    "        f_images= netG(z_noise)\n",
    "        dfake= netD(f_images.detach()).view(-1)\n",
    "        loss_D_fake= criterion(dfake, fake_label)\n",
    "        loss_D_fake.backward()\n",
    "\n",
    "        Dloss= loss_D_real+loss_D_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Update G with fake data\n",
    "        netG.zero_grad()\n",
    "        dfake= netD(f_images).view(-1)\n",
    "        loss_G = criterion(dfake, real_label)\n",
    "        loss_G.backward()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch {} [{}/{}] loss_D_real: {:.4f} loss_D_fake: {:.4f} loss_G: {:.4f}'.format(\n",
    "                epoch, i, len(dataloader),\n",
    "                loss_D_real.mean().item(),\n",
    "                loss_D_fake.mean().item(),\n",
    "                loss_G.mean().item()\n",
    "            ))\n",
    "            vutils.save_image(x_real, os.path.join(OUT_PATH, 'real_samples{}.png'.format(epoch)), normalize=True)\n",
    "            with torch.no_grad():\n",
    "                viz_sample = netG(viz_noise)\n",
    "                vutils.save_image(viz_sample, os.path.join(OUT_PATH, 'fake_samples_{}.png'.format(epoch)), normalize=True)\n",
    "    torch.save(netG.state_dict(), os.path.join(OUT_PATH, 'netG_{}.pth'.format(epoch)))\n",
    "    torch.save(netD.state_dict(), os.path.join(OUT_PATH, 'netD_{}.pth'.format(epoch)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
